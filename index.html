<!DOCTYPE html>
<html>
<head>
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Belanosima&family=Geologica:wght@400;700&family=Roboto+Mono&display=swap" rel="stylesheet">
	<title>My Website</title>
	<link rel="stylesheet" type="text/css" href="styles.css">
</head>
<body>
  <div class="container">
    <div class="top-bar">
      <a href="https://within-cells-interlink-ed.github.io/index.html">Home</a>
      <a href="https://within-cells-interlink-ed.github.io/categories.html">Categories</a>
      <a href="https://within-cells-interlink-ed.github.io/contacts.html">Contacts</a>
      <a href="https://within-cells-interlink-ed.github.io/about.html">About</a>
    </div>
    <img src="https://within-cells-interlink-ed.github.io/images/167541211125912.jpg" style="max-width: 96.7%; height: auto; display: block; margin-left: auto; margin-right: auto;">
    <h1>Heading 1</h1>
	<p>Count me among those who are alarmed about the implications of &quot;AI,&quot; such as it is. But I am not among those who worry about machines taking over. I see no signs of intelligence&mdash;either from the large language models being hyped right now, or from those doing the hyping. My concern around this technology is more mundane than apocalypse, but more profound than simple economic impact.</p>

	<p> I'm terrified we're about to lose the war for truth.</p>

	<p> TL;DR: The ability to generate massive amounts of language will be exploited for profit and weaponized to disseminate misinformation against a public that will naively consume this language because it is convenient. Over time, what is true will become indistinguishable from what is hallucinated.  A Unique Hacking Vector</p>

	<p> My job is to highlight threats to information security. Normally that takes the shape of vulnerabilities in software or system configurations that can be exploited to compromise the confidentiality, integrity, or availability of stored data.</p>

	<p> But before I did this, I taught computer science. Before that, I taught English. Put simply, I have spent my professional life using myriad languages to convey knowledge and intention to others. So have you, I suspect, but maybe you weren't as big a dork about the idea. Me? I've always been fascinated by the ability for us to transmit, however imperfectly, the lightning from one mind to another through language.</p>

	<p> I'm not sure humans have a lock on communicating ideas and intention, but we sure do a lot of it. Language in its many forms is core to the human experience. It makes us unimaginably powerful, and dizzyingly vulnerable.</p>

	<p> This is the vulnerability we need to talk about now. You are about to get hacked in a way you've never seen before. Maybe you already have been; maybe we all have been. The integrity of our knowledge is in mortal peril.</p>

	<p> When you read, listen to, or watch nonfiction, with few exceptions, you have an expectation of the material* being true. At least, you have an expectation that the creator has done their best to present facts in good faith. That's a fair definition of &quot;nonfiction,&quot; as far as I can figure. It's important to note here that the &quot;truth&quot; is contingent upon intention. It matters that the people who made the material you're consuming were not trying to deceive you. Their intent is to inform. It is that intent, along with a history of some degree of accuracy, that brings you to their work.</p>

	<p> What is the intent of text generated by a large language model (LLM)? Is it attempting to inform? Deceive? Does it have ulterior motives?</p>

	<p> Some thinkers have written in horror that we &quot;can't know&quot; the intentions of AI, or that they align with ours. This &quot;alignment problem&quot; has launched countless papers, thinkpieces, podcasts, and one very-well-publicized letter from futurists, oligarchs, and other people who command large audiences at places like Davos, Aspen, and SXSW.</p>

	<p> First of all, let's get some terms straight here. The alignment problem, such as it is, concerns Artificial General Intelligence, or AGI, which refers to an artificial intelligence capable of cognitive tasks including reasoning and remembering across multiple domains of knowledge.</p>

	<p> I want you to internalize this: There is no such thing as AGI in 2023. Even the most concerned writers and researchers on this topic place likely dates for the emergence of anything like AGI in the 2050s to 2060s. The out-of-control computer is a distraction from the immediate dangers posed by the rapid adoption of large language models.</p>

	<p> Large language models do not reason. Their &quot;training&quot; concerns the statistical analysis of massive amounts of text to determine likely correct textual responses based on a given input. This is a simplification, but the basic gist is there: text is ingested, &quot;tokenized&quot; to identify important terms and positions, and based on many rounds of training, the model is eventually &quot;fitted,&quot; meaning it reliably produces output that makes sense given the input.</p>

	<p> That this methodology produces output as &quot;human&quot; as it does is truly phenomenal. I am in legitimate awe of what has been created. Remember that awe is equal parts wonder and fear.</p>

	<p> So back to the question of intent. What meaning lies behind the text generated by LLMs?</p>

	<p> Answer: none beyond what you bring to it. The model itself is incapable of intent or meaning. It is not moral or immoral; it is unaware&mdash;incapable of awareness&mdash;of morality in any useful sense.</p>

	<p> We may need some metaphors here.</p>

	<p> * I'm going to be avoiding the word &quot;content&quot; for reasons I hope become clear.  A New Chinese Room for LLMs</p>

	<p> You can't get too far into philosophical conversations about the nature of Artificial Intelligence without encountering John Searle's 1980 thought experiment known as the &quot;Chinese Room.&quot; In it, Searle describes the following scenario: a locked room inside of which is a person and a computer programmed to respond to messages written in Chinese. Messages written in Chinese are slipped under the door to this room, and the person uses the computer to compose a response, which is slipped back under the door.</p>

	<p> People on the other side have the impression that the person inside the room &quot;knows Chinese,&quot; but of course they are simply repeating what they are told to say by the computer. The thesis here is that passing for human (e.g. the Turing Test) is a terrible heuristic for &quot;intelligence,&quot; such as it is.</p>

	<p> But this thought experiment needs a little updating to properly capture what's going on with LLMs like ChatGPT. Lemme give it a shot.</p>

	<p> This time, we're back in a locked room. Well, maybe more like a locked library&mdash;filled with millions of books written in Chinese. The chamber also has the following:</p>

	<p> Infinite sheets of paper  A pair of scissors  Infinite glue sticks  A very scared and confused person</p>

	<p> Once again, messages are passed into this chamber in Chinese. The person, who does not know Chinese, uses the materials at hand to craft &quot;ransom note&quot; style responses. If the response is intelligible to the recipients, food is provided. If the response is nonsense, an electric shock is administered.</p>

	<p> Over time, perhaps this poor individual would discover patterns of characters in the infinite books at their disposal, learning what characters are more likely to follow others. Given enough time and voltage, they may even reliably produce coherent responses. But has the person &quot;learned&quot; Chinese?</p>

	<p> No, because they have no flipping idea what they're saying. They have one goal: produce output that is rewarded. If there is meaning at all in their messages, it extends only as far as &quot;food please, not shockies.&quot;</p>

	<p> So it is with large language models. The language they produce is bereft of meaning besides that with which we, the reader, imbue it.</p>

	<p> For material produced by LLMs, meaning is in the eye of the beholder.</p>
  </div>

</body>
</html>
